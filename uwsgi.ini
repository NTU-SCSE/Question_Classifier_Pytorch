[uwsgi]
# placeholders that you have to change
my_app_folder = /home/michael/Question_Classifier_Pytorch
my_user = michael

socket = %(my_app_folder)/production_ml.sock
chdir = %(my_app_folder)
file = rest_api_server.py
callable = app

# environment variables
env = PYTHONPATH=%(my_app_folder):$PATH
env = LD_LIBRARY_PATH=$LD_LIBRARY_PATH
env = CUDA_HOME=$CUDA_HOME

master = true
processes = 1
# allows nginx (and all users) to read and write on this socket
chmod-socket = 666
# remove the socket when the process stops
vacuum = true

# loads your application one time per worker
# will very probably consume more memory,
# but will run in a more consistent and clean environment.
lazy-apps = true

uid = %(my_user)
gid = %(my_user)

# uWSGI will kill the process instead of reloading it
die-on-term = true
# socket file for getting stats about the workers
stats = %(my_app_folder)/stats.production_ml.sock

# Scaling the server with the Cheaper subsystem

# set cheaper algorithm to use, if not set default will be used
cheaper-algo = spare
# minimum number of workers to keep at all times
cheaper = 0
# number of workers to spawn at startup
cheaper-initial = 1
# maximum number of workers that can be spawned
workers = 1
# how many workers should be spawned at a time
heaper-step = 1

pyargv = --path rest_api_param.json

~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                                                                                                                                                                                                           
~                               
